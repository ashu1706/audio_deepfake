{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16f3c2ff-fa08-47a2-8da9-5dbb29701d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "\n",
    "# Initialize Wav2Vec processor and model for embeddings\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
    "model_wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
    "\n",
    "def extract_mfcc(file_path, n_mfcc=13):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    return np.mean(mfcc.T, axis=0)  # Taking the mean to get a fixed size feature vector\n",
    "\n",
    "def extract_mel_spectrogram(file_path, n_mels=128):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "    return mel_spectrogram_db\n",
    "\n",
    "def extract_wav2vec_large_features(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=16000)  # Wav2Vec expects a 16kHz sampling rate\n",
    "    input_values = feature_extractor(y, sampling_rate=16000, return_tensors=\"pt\").input_values\n",
    "    with torch.no_grad():\n",
    "        embeddings = model_wav2vec(input_values).last_hidden_state\n",
    "    return embeddings.mean(dim=1).squeeze().numpy()  # Use the mean of embeddings for a fixed-size vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fec30208-2982-4f08-8ef6-07ef5c91ea79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 582ms/step\n",
      "Prediction: Fake audio\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Define expected feature dimensions\n",
    "num_mfcc_features = 13  # Adjust based on your MFCC extraction configuration\n",
    "mel_spec_height = 128   # Expected height of Mel spectrogram\n",
    "mel_spec_width = 63     # Expected width of Mel spectrogram\n",
    "num_embeddings = 1024   # Expected size of Wav2Vec embeddings\n",
    "\n",
    "# Function to pad or truncate Mel spectrogram to fixed width\n",
    "def pad_or_truncate_mel_spec(mel_spec, target_height=128, target_width=63):\n",
    "    if mel_spec.shape[1] > target_width:  # Truncate if wider than target\n",
    "        mel_spec = mel_spec[:, :target_width]\n",
    "    else:  # Pad if narrower than target\n",
    "        padding = target_width - mel_spec.shape[1]\n",
    "        mel_spec = np.pad(mel_spec, ((0, 0), (0, padding)), mode='constant')\n",
    "    return mel_spec\n",
    "\n",
    "def preprocess_audio_for_prediction(file_path):\n",
    "    mfcc_features = extract_mfcc(file_path)  # Shape: (num_mfcc_features,)\n",
    "    mel_spectrogram_features = extract_mel_spectrogram(file_path)\n",
    "    wav2vec_features = extract_wav2vec_large_features(file_path)  # Shape: (num_embeddings,)\n",
    "\n",
    "    # Pad or truncate Mel spectrogram to (128, 63)\n",
    "    mel_spectrogram_features = pad_or_truncate_mel_spec(mel_spectrogram_features, target_height=mel_spec_height, target_width=mel_spec_width)\n",
    "\n",
    "    # Reshape features for the model input\n",
    "    mel_spectrogram_features = mel_spectrogram_features.reshape((1, mel_spec_height, mel_spec_width, 1))  # Add channel dimension\n",
    "    mfcc_features = mfcc_features.reshape((1, num_mfcc_features))\n",
    "    wav2vec_features = wav2vec_features.reshape((1, num_embeddings))\n",
    "    \n",
    "    return mfcc_features, mel_spectrogram_features, wav2vec_features\n",
    "\n",
    "# Load and test the model on a new audio sample\n",
    "sample_audio_file = r\"C:\\Users\\ACER\\Desktop\\AudioDF\\audio\\fake\\file24.wav_16k.wav_norm.wav_mono.wav_silence.wav_2sec.wav\"\n",
    "X_mfcc, X_mel, X_wav2vec = preprocess_audio_for_prediction(sample_audio_file)\n",
    "\n",
    "#tf.keras.saving.register_keras_serializable()\n",
    "def focal_loss_fixed(y_true, y_pred, gamma=2.0, alpha=0.25):\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
    "    loss = -y_true * alpha * tf.pow(1 - y_pred, gamma) * tf.math.log(y_pred) - (1 - y_true) * (1 - alpha) * tf.pow(y_pred, gamma) * tf.math.log(1 - y_pred)\n",
    "    return tf.reduce_mean(loss)\n",
    "# Load the model, specify custom_objects\n",
    "model = load_model('audio_deepfake_finetune_model1.h5', custom_objects={'focal_loss_fixed': focal_loss_fixed})\n",
    "\n",
    "# Re-save the model in `.keras` format\n",
    "#model.save('best_model.keras')\n",
    "\n",
    "#model = load_model('best_model.keras', custom_objects={'focal_loss_fixed': focal_loss_fixed})\n",
    "\n",
    "# Make a prediction\n",
    "prediction = model.predict([X_mfcc, X_mel, X_wav2vec])\n",
    "\n",
    "# Interpret and print the result\n",
    "if prediction[0] > 0.5:\n",
    "    print(\"Prediction: Fake audio\")\n",
    "else:\n",
    "    print(\"Prediction: Real audio\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf8f827-0707-4055-867c-c41e956ded1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
