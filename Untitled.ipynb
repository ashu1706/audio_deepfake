{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef6d45e2-ae09-46f2-b288-443c5b362a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6165ac30-2d30-47b1-8670-17149f65aa26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12aeec84-1650-4955-af7f-faa9679ee33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\acer\\anacond3\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\acer\\anacond3\\lib\\site-packages (4.47.0.dev0)\n",
      "Requirement already satisfied: librosa in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (0.10.2.post1)\n",
      "Requirement already satisfied: pandas in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (1.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from torch) (70.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\acer\\anacond3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\acer\\anacond3\\lib\\site-packages (from transformers) (0.26.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\acer\\anacond3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\acer\\anacond3\\lib\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\acer\\anacond3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from librosa) (1.14.0)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from pooch>=1.1->librosa) (4.2.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch transformers librosa pandas scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c7040ee-72b3-419c-9a87-211d2236e755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "def load_audio(file_path, sample_rate=16000):\n",
    "    # Load audio file with librosa\n",
    "    audio, _ = librosa.load(file_path, sr=sample_rate)\n",
    "    return audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a300d267-5a0c-4a68-9b3a-9dd3db59c69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_audio_from_folders(data_dir):\n",
    "    audio_data = []\n",
    "    audio_labels = []\n",
    "\n",
    "    for label in os.listdir(data_dir):\n",
    "        label_path = os.path.join(data_dir, label)\n",
    "        \n",
    "        # Skip non-directory files\n",
    "        if not os.path.isdir(label_path):\n",
    "            continue\n",
    "            \n",
    "        for file_name in os.listdir(label_path):\n",
    "            file_path = os.path.join(label_path, file_name)\n",
    "            \n",
    "            # Check if the file is a valid audio file (e.g., .wav, .mp3)\n",
    "            if not (file_name.endswith('.wav') or file_name.endswith('.mp3')):\n",
    "                continue\n",
    "            \n",
    "            # Load audio\n",
    "            audio = load_audio(file_path)  # Load with librosa\n",
    "            audio_data.append(audio)\n",
    "            audio_labels.append(label)\n",
    "\n",
    "    return audio_data, audio_labels\n",
    "\n",
    "# Example usage\n",
    "#audio_files, labels = load_audio_from_folders(\"Merged_Dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b1b0b74-32ce-40ef-8f17-e91a095ccbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\anacond3\\Lib\\site-packages\\transformers\\configuration_utils.py:306: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "import torch\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02dd53a4-9c86-4128-8a3d-428da4a56bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wav2vec_embedding(audio):\n",
    "    inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state\n",
    "        # Pool across time dimension to get a single embedding vector\n",
    "        embedding = torch.mean(embeddings, dim=1)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44ef1a1a-0c5b-4674-afce-eda80cfed788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def prepare_dataset(audio_files, labels):\n",
    "    embeddings = []\n",
    "    for i, file in enumerate(audio_files):\n",
    "        audio = load_audio(file)  # Load audio file\n",
    "        embedding = extract_wav2vec_embedding(audio)  # Extract embedding\n",
    "        embeddings.append(embedding.cpu().numpy())  # Append embedding to the list\n",
    "    return np.array(embeddings), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f1863c7-1188-49db-9bdf-ef6aefb48469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword argument `sampling_rate` is not a valid argument for this processor and will be ignored.\n",
      "Keyword argument `padding` is not a valid argument for this processor and will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (1088, 1, 768)\n",
      "Labels shape: (1088,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import librosa\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "\n",
    "# Load Wav2Vec model and processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "model.eval()\n",
    "\n",
    "def load_audio(file_path, sample_rate=16000):\n",
    "    # Load audio file with librosa\n",
    "    audio, _ = librosa.load(file_path, sr=sample_rate)\n",
    "    return audio\n",
    "\n",
    "def extract_wav2vec_embedding(audio):\n",
    "    inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state\n",
    "        # Pool across time dimension to get a single embedding vector\n",
    "        embedding = torch.mean(embeddings, dim=1)\n",
    "    return embedding\n",
    "\n",
    "def load_audio_from_folders(data_dir):\n",
    "    audio_data = []\n",
    "    audio_labels = []\n",
    "    \n",
    "    # Loop through each folder ('real' and 'fake')\n",
    "    for label_folder in os.listdir(data_dir):\n",
    "        label_path = os.path.join(data_dir, label_folder)\n",
    "        \n",
    "        if os.path.isdir(label_path):\n",
    "            label = 1 if label_folder == 'real' else 0\n",
    "            \n",
    "            for file_name in os.listdir(label_path):\n",
    "                file_path = os.path.join(label_path, file_name)\n",
    "                \n",
    "                # Append the file path and label\n",
    "                audio_data.append(file_path)\n",
    "                audio_labels.append(label)\n",
    "                \n",
    "    return audio_data, audio_labels\n",
    "\n",
    "def prepare_dataset(audio_files, labels):\n",
    "    embeddings = []\n",
    "    for i, file in enumerate(audio_files):\n",
    "        audio = load_audio(file)  # Load audio file\n",
    "        embedding = extract_wav2vec_embedding(audio)  # Extract embedding\n",
    "        embeddings.append(embedding.cpu().numpy())  # Append embedding to the list\n",
    "    return np.array(embeddings), np.array(labels)\n",
    "\n",
    "# Load your audio files and labels\n",
    "data_directory = r'C:\\Users\\ACER\\Desktop\\AudioDF\\audio'  # Update this to your dataset path\n",
    "audio_files, labels = load_audio_from_folders(data_directory)  # Ensure this function is defined\n",
    "\n",
    "# Prepare dataset to get embeddings and labels\n",
    "embeddings, labels = prepare_dataset(audio_files, labels)\n",
    "\n",
    "# Print the shape of embeddings\n",
    "print(\"Embeddings shape:\", embeddings.shape)  # Should be (num_samples, 768)\n",
    "print(\"Labels shape:\", labels.shape)          # Should be (num_samples,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "767b21c9-1c02-461e-b341-3fcb0d8c0433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "035fd2f6-ca7f-40dc-a51d-d0a5a501ebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.tensor(embeddings).float(), torch.tensor(labels).float())\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e0344bc-d65c-4476-8396-7b115690a1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 128)  # 768 is the output size of Wav2Vec embeddings\n",
    "        self.fc2 = nn.Linear(128, 1)     # Binary classification output\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "07a36ad3-c6de-43f3-8782-b1235492ece7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.6384\n",
      "Epoch [2/100], Loss: 0.4358\n",
      "Epoch [3/100], Loss: 0.2677\n",
      "Epoch [4/100], Loss: 0.1663\n",
      "Epoch [5/100], Loss: 0.1098\n",
      "Epoch [6/100], Loss: 0.0832\n",
      "Epoch [7/100], Loss: 0.0610\n",
      "Epoch [8/100], Loss: 0.0446\n",
      "Epoch [9/100], Loss: 0.0385\n",
      "Epoch [10/100], Loss: 0.0326\n",
      "Epoch [11/100], Loss: 0.0250\n",
      "Epoch [12/100], Loss: 0.0204\n",
      "Epoch [13/100], Loss: 0.0164\n",
      "Epoch [14/100], Loss: 0.0143\n",
      "Epoch [15/100], Loss: 0.0135\n",
      "Epoch [16/100], Loss: 0.0113\n",
      "Epoch [17/100], Loss: 0.0112\n",
      "Epoch [18/100], Loss: 0.0085\n",
      "Epoch [19/100], Loss: 0.0080\n",
      "Epoch [20/100], Loss: 0.0068\n",
      "Epoch [21/100], Loss: 0.0067\n",
      "Epoch [22/100], Loss: 0.0064\n",
      "Epoch [23/100], Loss: 0.0087\n",
      "Epoch [24/100], Loss: 0.0050\n",
      "Epoch [25/100], Loss: 0.0042\n",
      "Epoch [26/100], Loss: 0.0041\n",
      "Epoch [27/100], Loss: 0.0036\n",
      "Epoch [28/100], Loss: 0.0035\n",
      "Epoch [29/100], Loss: 0.0032\n",
      "Epoch [30/100], Loss: 0.0035\n",
      "Epoch [31/100], Loss: 0.0027\n",
      "Epoch [32/100], Loss: 0.0025\n",
      "Epoch [33/100], Loss: 0.0023\n",
      "Epoch [34/100], Loss: 0.0023\n",
      "Epoch [35/100], Loss: 0.0020\n",
      "Epoch [36/100], Loss: 0.0020\n",
      "Epoch [37/100], Loss: 0.0017\n",
      "Epoch [38/100], Loss: 0.0016\n",
      "Epoch [39/100], Loss: 0.0017\n",
      "Epoch [40/100], Loss: 0.0015\n",
      "Epoch [41/100], Loss: 0.0014\n",
      "Epoch [42/100], Loss: 0.0018\n",
      "Epoch [43/100], Loss: 0.0014\n",
      "Epoch [44/100], Loss: 0.0011\n",
      "Epoch [45/100], Loss: 0.0012\n",
      "Epoch [46/100], Loss: 0.0010\n",
      "Epoch [47/100], Loss: 0.0009\n",
      "Epoch [48/100], Loss: 0.0010\n",
      "Epoch [49/100], Loss: 0.0009\n",
      "Epoch [50/100], Loss: 0.0008\n",
      "Epoch [51/100], Loss: 0.0008\n",
      "Epoch [52/100], Loss: 0.0007\n",
      "Epoch [53/100], Loss: 0.0008\n",
      "Epoch [54/100], Loss: 0.0008\n",
      "Epoch [55/100], Loss: 0.0008\n",
      "Epoch [56/100], Loss: 0.0006\n",
      "Epoch [57/100], Loss: 0.0007\n",
      "Epoch [58/100], Loss: 0.0006\n",
      "Epoch [59/100], Loss: 0.0006\n",
      "Epoch [60/100], Loss: 0.0005\n",
      "Epoch [61/100], Loss: 0.0005\n",
      "Epoch [62/100], Loss: 0.0005\n",
      "Epoch [63/100], Loss: 0.0004\n",
      "Epoch [64/100], Loss: 0.0004\n",
      "Epoch [65/100], Loss: 0.0004\n",
      "Epoch [66/100], Loss: 0.0004\n",
      "Epoch [67/100], Loss: 0.0004\n",
      "Epoch [68/100], Loss: 0.0004\n",
      "Epoch [69/100], Loss: 0.0004\n",
      "Epoch [70/100], Loss: 0.0003\n",
      "Epoch [71/100], Loss: 0.0003\n",
      "Epoch [72/100], Loss: 0.0003\n",
      "Epoch [73/100], Loss: 0.0003\n",
      "Epoch [74/100], Loss: 0.0003\n",
      "Epoch [75/100], Loss: 0.0003\n",
      "Epoch [76/100], Loss: 0.0003\n",
      "Epoch [77/100], Loss: 0.0002\n",
      "Epoch [78/100], Loss: 0.0003\n",
      "Epoch [79/100], Loss: 0.0002\n",
      "Epoch [80/100], Loss: 0.0002\n",
      "Epoch [81/100], Loss: 0.0002\n",
      "Epoch [82/100], Loss: 0.0002\n",
      "Epoch [83/100], Loss: 0.0002\n",
      "Epoch [84/100], Loss: 0.0002\n",
      "Epoch [85/100], Loss: 0.0002\n",
      "Epoch [86/100], Loss: 0.0002\n",
      "Epoch [87/100], Loss: 0.0001\n",
      "Epoch [88/100], Loss: 0.0001\n",
      "Epoch [89/100], Loss: 0.0001\n",
      "Epoch [90/100], Loss: 0.0001\n",
      "Epoch [91/100], Loss: 0.0001\n",
      "Epoch [92/100], Loss: 0.0001\n",
      "Epoch [93/100], Loss: 0.0001\n",
      "Epoch [94/100], Loss: 0.0001\n",
      "Epoch [95/100], Loss: 0.0001\n",
      "Epoch [96/100], Loss: 0.0001\n",
      "Epoch [97/100], Loss: 0.0001\n",
      "Epoch [98/100], Loss: 0.0001\n",
      "Epoch [99/100], Loss: 0.0001\n",
      "Epoch [100/100], Loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AudioClassifier().to(device)\n",
    "criterion = nn.BCELoss()  # Use appropriate loss function for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels)  # Squeeze if needed\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d8260906-a169-4708-974f-5063e2bca5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_audio(audio):\n",
    "    # Example: pitch shifting and adding noise\n",
    "    audio_augmented = librosa.effects.pitch_shift(audio, sr=16000, n_steps=4)\n",
    "    noise = np.random.randn(len(audio))\n",
    "    audio_augmented = audio + 0.005 * noise\n",
    "    return audio_augmented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "215d523f-d7d8-43c9-be4e-1527ed8ce185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def cluster_embeddings(embeddings, n_clusters=2):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    clusters = kmeans.fit_predict(embeddings)\n",
    "    return clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "75c6f3ee-b9af-43c4-90fb-275b4683e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"deepfake_audio_detection_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "884c03a3-dfda-4ca3-9c0f-b9fc63b53518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for the audio sample: Real\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "\n",
    "# Load your models\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "wav2vec2_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "model = AudioClassifier()  # Your custom classifier\n",
    "model.load_state_dict(torch.load(\"deepfake_audio_detection_model.pth\", weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "def load_and_preprocess_audio(audio_path):\n",
    "    audio, sr = librosa.load(audio_path, sr=16000)\n",
    "    inputs = processor(audio, return_tensors=\"pt\", padding=True, sampling_rate=16000)\n",
    "    return inputs['input_values']\n",
    "\n",
    "def predict(audio_path):\n",
    "    inputs = load_and_preprocess_audio(audio_path)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Extract features using Wav2Vec2\n",
    "        features = wav2vec2_model(inputs).last_hidden_state\n",
    "        pooled_features = torch.mean(features, dim=1)  # Mean pooling\n",
    "        \n",
    "        outputs = model(pooled_features)  # Pass pooled features to the classifier\n",
    "\n",
    "    predictions = torch.sigmoid(outputs)  # Adjust based on your model's output\n",
    "    return predictions.numpy()\n",
    "\n",
    "# Test on an audio sample\n",
    "audio_file = r'audio/fake/file1018.wav_16k.wav_norm.wav_mono.wav_silence.wav_2sec.wav'\n",
    "predictions = predict(audio_file)\n",
    "\n",
    "# Assuming binary classification\n",
    "label = \"Real\" if predictions[0] > 0.5 else \"Fake\"\n",
    "print(f\"Prediction for the audio sample: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1dc24d-e163-4560-ba9f-90eb4ec2b1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def create_model(learning_rate=0.001, dropout_rate=0.5):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 63, 1)))\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # For binary classification\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "class KerasClassifierWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, learning_rate=0.001, dropout_rate=0.5, epochs=10, batch_size=32):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.model = None\n",
    "\n",
    "    def create_model(self):\n",
    "        return create_model(self.learning_rate, self.dropout_rate)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model = self.create_model()\n",
    "        # Adding EarlyStopping\n",
    "        early_stopping = EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n",
    "        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.model.predict(X) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.01],\n",
    "    'dropout_rate': [0.3, 0.5],\n",
    "    'batch_size': [8, 16, 32],\n",
    "    'epochs': [5, 10]\n",
    "}\n",
    "\n",
    "# Now you can use the custom wrapper in GridSearchCV\n",
    "model = KerasClassifierWrapper()\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', n_jobs=-1, cv=3)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_result = grid.fit(X_mel_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters found: \", grid_result.best_params_)\n",
    "print(\"Best accuracy: \", grid_result.best_score_)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca2df6d-34bc-4f36-9045-143f624925c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "class KerasClassifierWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, learning_rate=0.001, dropout_rate=0.5, epochs=10, batch_size=8):  # Reduced batch size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.model = None\n",
    "\n",
    "    def create_model(self):\n",
    "        model = tf.keras.models.Sequential()\n",
    "        model.add(tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(128, 63, 1)))  # Reduced number of filters\n",
    "        model.add(tf.keras.layers.Dropout(self.dropout_rate))\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "        model.add(tf.keras.layers.Dense(32, activation='relu'))  # Reduced number of neurons\n",
    "        model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # For binary classification\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model = self.create_model()\n",
    "        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=1)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.model.predict(X) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Now you can use the custom wrapper in GridSearchCV\n",
    "model = KerasClassifierWrapper()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.01],\n",
    "    'dropout_rate': [0.3, 0.5],\n",
    "    'batch_size': [8, 16],  # Reduced batch size\n",
    "    'epochs': [5, 10]  # Reduced epochs\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object with n_jobs=1\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', n_jobs=1, cv=3)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_result = grid.fit(X_mel_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters found: \", grid_result.best_params_)\n",
    "print(\"Best accuracy: \", grid_result.best_score_)\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
